{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4befd3e-b1ca-4d08-9943-cecdf6dfcf95",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "From [this](https://flax.readthedocs.io/en/v0.6.10/guides/jax_for_the_impatient.html#full-example-linear-regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a98819f-fdda-4950-b995-f3dea43f5f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6507166-e63e-441f-95e7-791312bbe987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set problem dimensions.\n",
    "n_samples = 20\n",
    "x_dim = 10\n",
    "y_dim = 5\n",
    "\n",
    "# Generate random ground truth W and b.\n",
    "key = random.PRNGKey(0)\n",
    "k1, k2 = random.split(key)\n",
    "W = random.normal(k1, (x_dim, y_dim))\n",
    "b = random.normal(k2, (y_dim,))\n",
    "\n",
    "# Generate samples with additional noise.\n",
    "key_sample, key_noise = random.split(k1)\n",
    "x_samples = random.normal(key_sample, (n_samples, x_dim))\n",
    "y_samples = x_samples @ W + b + 0.1 * random.normal(key_noise, (n_samples, y_dim))\n",
    "print(\"x shape:\", x_samples.shape, \"; y shape:\", y_samples.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52865fb7-61f7-4dbd-9142-46da1d7508d4",
   "metadata": {},
   "source": [
    "## Without pytrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9670df8c-7bb2-47ed-be0a-da1237adf193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function: Mean squared error.\n",
    "@jax.jit\n",
    "def mse(W, b, x_batched, y_batched):\n",
    "    y_hat = x_batched @ W + b  # returns NxP\n",
    "    err = 0.5 * jnp.sum((y_hat - y_batched) ** 2, axis=1)  # NxP, NxP -> (N, )\n",
    "    return jnp.mean(err)  # (N, ) -> ()\n",
    "\n",
    "\n",
    "# Ensure we jit the largest-possible jittable block.\n",
    "@jax.jit\n",
    "def update_params(W, b, x, y, lr):\n",
    "    # we can get the loss for free with autodiff\n",
    "    loss, grad = jax.value_and_grad(mse, argnums=(0, 1))(W, b, x, y)\n",
    "    W, b = W - lr * grad[0], b - lr * grad[1]\n",
    "    return W, b, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e1c3e4-0132-4eb7-9877-42bce0836506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize estimated W and b with zeros.\n",
    "W_hat = jnp.zeros_like(W)\n",
    "b_hat = jnp.zeros_like(b)\n",
    "\n",
    "learning_rate = 0.3  # Gradient step size.\n",
    "print('Loss for \"true\" W,b: ', mse(W, b, x_samples, y_samples))\n",
    "for i in range(101):\n",
    "    W_hat, b_hat, loss = update_params(\n",
    "        W_hat, b_hat, x_samples, y_samples, learning_rate\n",
    "    )\n",
    "    if i % 5 == 0:\n",
    "        # printing the loss introduces overhead of the device syncing with the host (Python process)\n",
    "        print(f\"Loss step {i}: \", loss)\n",
    "print(f\"\\nW:{W}\\nW_hat:{W_hat}\\n\\nb:{b}\\nb_hat:{b_hat}\")\n",
    "print(\"Diff:\", jnp.linalg.norm(W - W_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e6e524-c382-4e76-ae34-955cfe853396",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SOLUTION\n",
    "\"\"\"\n",
    "# Linear feed-forward.\n",
    "def predict(W, b, x):\n",
    "  return jnp.dot(x, W) + b\n",
    "\n",
    "# Loss function: Mean squared error.\n",
    "def mse(W, b, x_batched, y_batched):\n",
    "  # Define the squared loss for a single pair (x,y)\n",
    "  def squared_error(x, y):\n",
    "    y_pred = predict(W, b, x)\n",
    "    return jnp.inner(y-y_pred, y-y_pred) / 2.0\n",
    "  # We vectorize the previous to compute the average of the loss on all samples.\n",
    "  return jnp.mean(jax.vmap(squared_error)(x_batched, y_batched), axis=0)\n",
    "\n",
    "# Set problem dimensions.\n",
    "n_samples = 20\n",
    "x_dim = 10\n",
    "y_dim = 5\n",
    "\n",
    "# Generate random ground truth W and b.\n",
    "key = random.PRNGKey(0)\n",
    "k1, k2 = random.split(key)\n",
    "W = random.normal(k1, (x_dim, y_dim))\n",
    "b = random.normal(k2, (y_dim,))\n",
    "\n",
    "# Generate samples with additional noise.\n",
    "key_sample, key_noise = random.split(k1)\n",
    "x_samples = random.normal(key_sample, (n_samples, x_dim))\n",
    "y_samples = predict(W, b, x_samples) + 0.1 * random.normal(key_noise,(n_samples, y_dim))\n",
    "print('x shape:', x_samples.shape, '; y shape:', y_samples.shape)\n",
    "\n",
    "# Initialize estimated W and b with zeros.\n",
    "W_hat = jnp.zeros_like(W)\n",
    "b_hat = jnp.zeros_like(b)\n",
    "\n",
    "# Ensure we jit the largest-possible jittable block.\n",
    "@jax.jit\n",
    "def update_params(W, b, x, y, lr):\n",
    "  W, b = W - lr * jax.grad(mse, 0)(W, b, x, y), b - lr * jax.grad(mse, 1)(W, b, x, y)\n",
    "  return W, b\n",
    "\n",
    "learning_rate = 0.3  # Gradient step size.\n",
    "print('Loss for \"true\" W,b: ', mse(W, b, x_samples, y_samples))\n",
    "for i in range(101):\n",
    "  # Perform one gradient update.\n",
    "  W_hat, b_hat = update_params(W_hat, b_hat, x_samples, y_samples, learning_rate)\n",
    "  if (i % 5 == 0):\n",
    "    print(f\"Loss step {i}: \", mse(W_hat, b_hat, x_samples, y_samples))\n",
    "\"\"\"\n",
    "print(\"Orginal solution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f4214c-21b5-4dd1-88de-eadb368f6166",
   "metadata": {},
   "source": [
    "## With pytrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2484e3cd-1b20-49a1-8fed-7a755175dd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function: Mean squared error.\n",
    "@jax.jit\n",
    "def mse(params, x_batched, y_batched):\n",
    "    y_hat = x_batched @ params[\"W\"] + params[\"b\"]  # returns NxP\n",
    "    err = 0.5 * jnp.sum((y_hat - y_batched) ** 2, axis=1)  # NxP, NxP -> (N, )\n",
    "    return jnp.mean(err)  # (N, ) -> ()\n",
    "\n",
    "\n",
    "# Ensure we jit the largest-possible jittable block.\n",
    "@jax.jit\n",
    "def update_params(params, x, y, lr):\n",
    "    # we can get the loss for free with autodiff\n",
    "    loss, grad = jax.value_and_grad(mse)(params, x, y)\n",
    "    params = jax.tree_util.tree_map(lambda p, g: p - lr * g, params, grad)\n",
    "    return params, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8300b381-577a-4476-8ff6-f48200c99e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"W\": jnp.zeros_like(W), \"b\": jnp.zeros_like(b)}\n",
    "\n",
    "learning_rate = 0.3  # Gradient step size.\n",
    "print('Loss for \"true\" W,b: ', mse(params, x_samples, y_samples))\n",
    "for i in range(101):\n",
    "    # need to pipe the params in and out of the update because JAX works\n",
    "    # with stateless computations https://docs.jax.dev/en/latest/stateful-computations.html\n",
    "    params, loss = update_params(params, x_samples, y_samples, learning_rate)\n",
    "    if i % 5 == 0:\n",
    "        # printing the loss introduces overhead of the device syncing with the host (Python process)\n",
    "        print(f\"Loss step {i}: \", loss)\n",
    "print(f\"\\nW:{W}\\nW_hat:{W_hat}\\n\\nb:{b}\\nb_hat:{b_hat}\")\n",
    "print(\"Diff:\", jnp.linalg.norm(W - W_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cfe3aa-c39a-4d1c-9e7f-e54b9a6bc438",
   "metadata": {},
   "source": [
    "## Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed14b6e-7905-4b7c-9851-584328b6da25",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_samples_ones_stacked = jnp.c_[x_samples, jnp.ones(x_samples.shape[0])]\n",
    "assert x_samples_ones_stacked.shape == (x_samples.shape[0], x_samples.shape[1] + 1)\n",
    "W_hat_b_hat = jnp.linalg.lstsq(x_samples_ones_stacked, y_samples)[0]\n",
    "W_hat = W_hat_b_hat[:-1, :]\n",
    "print(\"Diff:\", jnp.linalg.norm(W - W_hat))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxstuff",
   "language": "python",
   "name": "jaxstuff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
