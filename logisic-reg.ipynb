{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0a54773-9d41-4c08-a97e-cb97f225961c",
   "metadata": {},
   "source": [
    "# Logistic regression and EF\n",
    "From [this](https://blackjax-devs.github.io/sampling-book/models/logistic_regression.html) and [this](https://blackjax-devs.github.io/sampling-book/models/LogisticRegressionWithLatentGaussianSampler.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78dffcd-89d9-466f-81df-1ed0ddfb5548",
   "metadata": {},
   "source": [
    "## Prepare the data and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808c70d5-91f7-46ac-a2d9-4a85b0919fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date\n",
    "\n",
    "rnd_state = int(date.today().strftime(\"%Y%m%d\"))\n",
    "rng_key = jax.random.key(rnd_state)\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "import blackjax\n",
    "\n",
    "# Variance of the Gaussian prior.\n",
    "# Smaller sigmas mean higher regularisation, i.e. we give more\n",
    "# importance to the prior and a bit less to the likelihood (the\n",
    "# data that is).\n",
    "SIGMA = 2\n",
    "\n",
    "## MCMC\n",
    "# variance of the proposal distribution (Gaussian) in the Metropolis\n",
    "# MCMC algorithm. This is proportional to the distance travelled in\n",
    "# sampling the posterior, but TAU too high results in \"sticky\" chains.\n",
    "TAU = 0.005\n",
    "CHAINS = 5_000\n",
    "BURNIN = 500\n",
    "\n",
    "## DIMENSIONS\n",
    "# Datapoints\n",
    "N = 50\n",
    "# Dimesions of the problem (dimensions of w not X)\n",
    "D = 2 + 1  # need to add the bias term\n",
    "\n",
    "# initialise the weigths\n",
    "rng_key, init_key = jax.random.split(rng_key)\n",
    "w0 = jax.random.multivariate_normal(init_key, -jnp.ones(D), jnp.eye(D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ee8cf6-d82f-4e1c-9eb7-578364aad0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PREDICTIONS\n",
    "def construct_prediction_points_mesh(x_train):\n",
    "    \"\"\"Prediction points constructed using a mesh grid.\"\"\"\n",
    "    xmin, ymin = x_train.min(axis=0) - 0.1\n",
    "    xmax, ymax = x_train.max(axis=0) + 0.1\n",
    "    step = 0.05\n",
    "    return np.mgrid[xmin:xmax:step, ymin:ymax:step]\n",
    "\n",
    "\n",
    "def push_predictions_forward(x_predict, weights_posterior):\n",
    "    _, nx, ny = x_predict.shape\n",
    "    phi_predict = jnp.concatenate([jnp.ones((1, nx, ny)), x_predict])\n",
    "    # perform push forward: sigma(y * Phi * w) with y = +1 or -1 for 2nd or 1st class\n",
    "    return jax.nn.sigmoid(jnp.einsum(\"dxy,sd->sxy\", phi_predict, weights_posterior))\n",
    "\n",
    "\n",
    "def plot_prediction_axis(x_predict, x_train, probs_2nd_class, ax):\n",
    "    ax.set_xlabel(r\"$X_0$\")\n",
    "    ax.set_ylabel(r\"$X_1$\")\n",
    "    cf = ax.contourf(*x_predict, probs_2nd_class)\n",
    "    cbar = fig.colorbar(cf, ax=ax)\n",
    "    cbar.set_label(\"P(y = +1 | x)\")\n",
    "    ax.scatter(*x_train.T, c=colors)\n",
    "\n",
    "\n",
    "def plot_predictions_2nd_class(x_train, weights_posterior, ax):\n",
    "    \"\"\"Plot the avg probability to belong to the 2nd cluster at each point in a grid.\"\"\"\n",
    "    x_predict = construct_prediction_points_mesh(x_train)\n",
    "    probs_2nd_class = push_predictions_forward(x_predict, weights_posterior)\n",
    "    # avg all the MCMC samples (only one chain here for now)\n",
    "    plot_prediction_axis(x_predict, x_train, probs_2nd_class.mean(axis=0), ax)\n",
    "\n",
    "\n",
    "def plot_one_prediction_2nd_class(x_train, weights_posterior, sample2plot, ax):\n",
    "    x_predict = construct_prediction_points_mesh(x_train)\n",
    "    probs_2nd_class = push_predictions_forward(x_predict, weights_posterior)\n",
    "    weights, probs_2nd_class = (\n",
    "        weights_posterior[sample2plot, :],\n",
    "        probs_2nd_class[sample2plot, :],\n",
    "    )\n",
    "    plot_prediction_axis(x_predict, x_train, probs_2nd_class, ax)\n",
    "    ax.set_title(f\"Weights {weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4242654d-3ff9-4006-bb0f-a4678315fcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DATA\n",
    "X, y = make_blobs(\n",
    "    N,\n",
    "    2,\n",
    "    centers=((-3, -3), (3, 3)),\n",
    "    cluster_std=1.5,\n",
    "    random_state=rnd_state,\n",
    ")\n",
    "# this is convenient for the likelihood formulation\n",
    "y = jnp.where(y, y, -1)  # convert into jax arrays\n",
    "colors = [\"tab:red\" if el > 0 else \"tab:blue\" for el in y]\n",
    "plt.scatter(*X.T, edgecolors=colors, c=\"none\")  # transpose to unpack last axis (2 dim)\n",
    "plt.xlabel(r\"$X_0$\")\n",
    "plt.ylabel(r\"$X_1$\")\n",
    "plt.show()\n",
    "assert np.nonzero(y)[0].shape == y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9d315b-608d-4078-9687-3b2cb726c444",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9bd63a-2e35-418b-9048-8d24eac88bfe",
   "metadata": {},
   "source": [
    "The labels $y$ are drawn from a Bernouilli distribution with probability $p$\n",
    "$$y \\sim \\mathrm{Bern}(p) $$\n",
    "\n",
    "and $p$ is computed using an exponential family model following a logistic regression likelihood\n",
    "$$p = \\text{log}\\left( \\Phi(X) w \\right)$$\n",
    "with $w$ being the weigts and $\\Phi(X)$ a function of the data $X$.\n",
    "\n",
    "**Dimensions.** There is one label for each 2D datapoint.\n",
    "1. $X$ has dimensions NxD\n",
    "2. $\\Phi(X)$ has dimensions NxD\n",
    "3. $w$ has dimensions D\n",
    "4. $p$ has dimensions N\n",
    "\n",
    "and $D=2$.\n",
    "\n",
    "### Exponential family and NN\n",
    "We want to predict the class $c$ for $X$, using the Bayes' theorem and assuming a likelihood being of the exponential family.\n",
    "\n",
    "Assumptions:\n",
    "1. **Bayes' theorem.** Using Bayes' theorem, we assign a finite amount of mass (probability) to both classes $c_1$ and $c_2$ and write down the posterior distribution for $w_1$\n",
    "$$ p(c = c_1 | X) = \\frac{p(X | c_1) p(c_1)}{p(X | c_1) p(c_1) + p(X | c_2) p(c_2) }.$$\n",
    "If we define $a_1 = \\mathrm{log}\\,\\frac{p(X | c_1) p(c_1)}{p(X | c_2) p(c_2)}$ then the posterior becomes\n",
    "$$ p(c = c_1 | X) = \\frac{1}{1 + \\mathrm{exp}(-a_1)} = \\sigma(a)$$\n",
    "with $\\sigma(.)$ called the logistic sigmoid function and $a_1$ is called the logit function for the class $c_1$ (aka activation).\n",
    "Note that $a_1$ represents a log ratio of the two possible classes.\n",
    "\n",
    "2. **EF.** Assume now that the probability $p(X | c = c_1)$ is an exponential family (EF), meaning:\n",
    "$$p(X | c = c_1) = \\text{exp}\\left[ \\Phi(X) w_1 - g(w_1) + h(X) \\right]$$\n",
    "with $ \\Phi(X) $ being the sufficient statistics, $g(w_1)$ the log of normalisation function and $h(x)$ the base measure, the latter assumed to be zero.\n",
    "\n",
    "Given 1 and 2, what is the log odd $a_1$?\n",
    "Recall that $a_1 = \\mathrm{log}\\,\\frac{p(X | c_1) p(c_1)}{p(X | c_2) p(c_2)}$ from assumption 1. Given assumption 2 (EF), $a_1$ becomes \n",
    "$$ a_1 = \\Phi(X) \\left( w_1 - w_2 \\right ) + g(w_1) - g(w_2) + \\text{log}\\,p(c_1) - \\text{log}\\,p(c_2)$$\n",
    "and thus the input of the logistic function $a$ will be a **linear classifier** in the natural parameters $w$.\n",
    "Therefore, we get $\\sigma(\\text{linear } a_1)$, where $a_1$ is a product between some weights and the features $\\Phi$ which describe the exponential famility we are considering.\n",
    "\n",
    "Note that this is the same idea of the last layer of a neural network with cross-entropy loss! Indeed, \n",
    "$$ a_1 = \\Phi(X) \\left( w_1 - w_2 \\right ) + g(w_1) - g(w_2) + \\text{log}\\,p(c_1) - \\text{log}\\,p(c_2)$$\n",
    "becomes\n",
    "$$\\Phi(X)\\theta + b$$\n",
    "with all the terms in $w$ grouped into $\\theta$ and the bias representing the priors.\n",
    "The big difference is in the choice of $\\Phi$: in EF, $\\Phi$ are the summary statistics and ensure that the class distribution is a probability distribution. In deep learning, this is not the case: can pick any $\\Phi$ don't need to be normalised that they sum to 1. Actually, $\\Phi$ comes from the lower layers, not normalised.\n",
    "\n",
    "This holds also in multiple classes, with the logistic function becoming a softmax and $a_k = \\Phi(X) w_k + g(w_k) + \\text{log}\\, p(c_k)$.\n",
    "There is one linear classifier for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb506530-0fe1-48a9-b2cf-0c63dfdfa7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi = jnp.c_[jnp.ones((N, 1)), X]\n",
    "assert Phi.shape == (N, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07112d10-25ab-419f-bebb-b519d9e6c34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logdensity(w, alpha=1.0, sigma=SIGMA):\n",
    "    \"\"\"The log-probability density function of the posterior distribution of the model up to a norm constant.\"\"\"\n",
    "    log_prior = -alpha * w @ w / (2 * sigma**2)  # log Gaussian prior\n",
    "    logits = Phi @ w  # linear comb of Gaussian is Gaussian\n",
    "    log_likelihood = jax.nn.log_sigmoid(y * logits)\n",
    "\n",
    "    return log_prior + log_likelihood.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972cb08e-162e-4009-b7b6-9df6cd0fc6b9",
   "metadata": {},
   "source": [
    "Next different algorithms are presented to find the optimal hyperplane separating the datapoints into two classes.\n",
    "Compared to a Gaussian regression, logistic regression has no closed form solution and approximations are then required.\n",
    "\n",
    "However, has fewer parameters (TODO from p X in Bishop): TODO.\n",
    "\n",
    "The following are considered here:\n",
    "1. Iterative reweighted least squares aka Newton-Raphson (this is the implementation in `R`?)\n",
    "2. Laplace approximation\n",
    "3. MacKay's approximation\n",
    "4. Random-walk Metropolis Hasting kernel MCMC\n",
    "\n",
    "This should work also with a Poisson likelihood regression and not only classification (Bernouilli)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2e27c5-3dc7-4094-b5c7-493edd567f78",
   "metadata": {},
   "source": [
    "## Algo 1: Iterative reweighted least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65f797c-65d4-4f6e-b9c3-9a30a6796b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c469dea4-d11d-4362-ae4c-a2e573e8788b",
   "metadata": {},
   "source": [
    "## Algo 4: Random-walk Metropolis MCMC\n",
    "Use a Normal distribution with covariance $\\tau^2 I_M$, where $\\tau$ is then a \"step\" size.\n",
    "The average distance travelled is proportional to $\\tau$ and the number of dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1b87d7-4d0c-4977-9028-6dd812f0f860",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmh = blackjax.rmh(logdensity, blackjax.mcmc.random_walk.normal(jnp.ones(D) * TAU))\n",
    "initial_state = rmh.init(w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e12eae-9f57-46e2-b4b2-8fe26d23f174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scan :: (f, (c, [a])) -> (c, [b])\n",
    "# f :: (c, a) -> (c, b)\n",
    "def inference_loop(rng_key, kernel, initial_state, num_samples):\n",
    "    @jax.jit  # this is not necessary as scan compiles the fn\n",
    "    def one_step(state, rng_key):  # f :: (c, a) -> (c, b)\n",
    "        state, info = kernel(rng_key, state)\n",
    "        return state, (state, info)\n",
    "\n",
    "    keys = jax.random.split(rng_key, num_samples)\n",
    "    # (f, (c, [a])) -> (c, [b])\n",
    "    _, (states, infos) = jax.lax.scan(one_step, initial_state, keys)\n",
    "    # note that b is a PyTree (per-step output) so JAX will stack each leaf\n",
    "    # across time, which looks like a transpose from “sequence of pytrees”\n",
    "    # to “pytree of sequences”. So instead of\n",
    "    # [(state0, info0), (state1, info1), ...] (a “list/array of tuples”) JAX\n",
    "    # returns (after stacking): ([state0, state1, ...], [info0, info1, ...])\n",
    "\n",
    "    return states, infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472b24ce-1d72-42bb-b39c-4bc4077c7d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key, sample_key = jax.random.split(rng_key)\n",
    "states, infos = inference_loop(sample_key, rmh.step, initial_state, CHAINS)\n",
    "rmh_weights = states.position[BURNIN:, :]\n",
    "print(infos.acceptance_rate.mean())\n",
    "infos.acceptance_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09a0592-d81a-4787-80a9-660af2933eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(12, 2))\n",
    "for i, axi in enumerate(ax):\n",
    "    # axi.plot(np.c_[np.ones_like(states.position[:, i]) * w0[i], states.position[:, i]])\n",
    "    axi.plot(states.position[:, i])\n",
    "    # as we concat first ones and then data, the first weight is the bias\n",
    "    axi.set_title(f\"$w_{i}$\" if i else \"b\")\n",
    "    axi.axvline(x=BURNIN, c=\"tab:red\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, layout=\"tight\")\n",
    "plot_predictions_2nd_class(X, rmh_weights, ax)\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, layout=\"tight\")\n",
    "sample2plot = 20\n",
    "plot_one_prediction_2nd_class(X, rmh_weights, sample2plot, ax)\n",
    "print(states.logdensity[BURNIN + sample2plot])\n",
    "print(states.logdensity[BURNIN:].mean())\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxstuff",
   "language": "python",
   "name": "jaxstuff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
